---
output: html_document
---

## Embedding visualization 

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(Rtsne)
library(ggplot2)
library(corrplot)
```

## Get embedding data for other models 

```{r import embeddings}
# GPT with 1536 embedding dimension - fixing the import 

## issue1: the raw data has embedding of each subject divided into two rows, the first row has data points squished into one cell and the second row has data points separates into respective cell.
## issue2: when the data points are made into list for each row and combined, each subject has different data points (some subjects have 1535 embedding results...)  

data_gpt1536_raw = read.csv("./data2/data_wide_embedding_gpt1536.csv") |>
  janitor::clean_names() |>
  mutate(pair_id = rep(1:(n() / 2), each = 2))

demo_gpt = data_gpt1536_raw |>
  filter(row_number() %% 2 == 1) |>
  select(seqn, gender, age, race, education, married, pir, bmi, pair_id)

squished_rows = data_gpt1536_raw |>
  filter(row_number() %% 2 == 1) |>
  select(embedding, pair_id)

squished_list = squished_rows |>
  mutate(
    embedding_vec = map(embedding, function(x) {
      if (is.na(x)) return(NA)
      clean_str = str_remove_all(x, "\\[|\\]") |> 
        str_trim()
      
      if (nchar(clean_str) == 0) return(numeric(0))
      vals = str_split(clean_str, ",\\s*")[[1]]
      num_vals = suppressWarnings(as.numeric(vals))
      
      return(num_vals)
    })
  ) |>
  filter(!map_lgl(embedding_vec, function(v) length(v) == 1 && is.na(v))) |>
  select(-embedding)

extended_rows = data_gpt1536_raw |>
  filter(row_number() %% 2 == 0) |>
  select(-x) |>
  mutate(across(everything(), ~ str_remove(.x, "]\"$"))) 

colnames(extended_rows)[-which(colnames(extended_rows) == "pair_id")] = 
  paste0("x", seq_len(ncol(extended_rows) - 1))

extended_rows = extended_rows |>
  mutate(across(everything(), ~ as.numeric(.x))) 

extended_list = extended_rows |>
  group_by(pair_id) |>
  summarize(embedding_vec2 = list(as.numeric(unlist(across(where(is.numeric)))))
  )

demo_squish = demo_gpt |>
  left_join(squished_list, by = "pair_id")
  
data_gpt1536_long = demo_squish |>
  left_join(extended_list, by = "pair_id") |>
  mutate(c_embedding = map2(embedding_vec, embedding_vec2, c)) |>
  select(-embedding_vec, -embedding_vec2) |>
  unnest(c_embedding) |>
  group_by(seqn) |>
  mutate(var_id = row_number()) |>
  ungroup()

data_gpt1536 = data_gpt1536_long |>
  pivot_wider(
    names_from = var_id, 
    values_from = c_embedding, 
    names_prefix = "var"
  ) |>
  select(seqn, gender, age, race, education, married, pir, bmi, var1:var1536)
  
#test1 = data_gpt1536_long |>
  #group_by(seqn) |>
  #summarize(n = n())



# GPT with 50 embedding dimension
data_gpt50 = read.csv("./data2/data_wide_embedding_gpt50.csv") |>
  janitor::clean_names() |>
  dplyr::select(-combined,-n_tokens)


# BERT with 768 embedding dimension
data_bert768 = read.csv("./data2/data_wide_embedding_bert768.csv") |>
  janitor::clean_names() |>
  dplyr::select(-x,-combined)

data_bert768$embedding = gsub("\\[", "", data_bert768$embedding)
data_bert768$embedding = gsub("\\]", "", data_bert768$embedding)
data_bert768$embedding = gsub("\n", " ", data_bert768$embedding)

data_bert768 = data_bert768 |>
  mutate(embedding = str_trim(embedding),  # Remove leading and trailing spaces
         embedding = str_replace_all(embedding, "\\s+", " ")) |> # Replace multiple spaces with a single space 
  separate(embedding, into = paste0("var", 1:768), sep = "\\s+", convert = TRUE) |>
  mutate(across(starts_with("var"), as.numeric))


# BERT with 50 embedding dimension
data_bert50 = read.csv("./data2/data_wide_embedding_bert50.csv") %>% 
  janitor::clean_names() |>
  dplyr::select(-x,-combined,-n_tokens)


# Cohere with 1024 embedding dimension
data_cohere1024 = read.csv("./data2/data_wide_embedding_cohere1024.csv") |>
  janitor::clean_names() |>
  dplyr::select(-x,-n_tokens)

data_cohere1024$embedding = gsub("\\[", "", data_cohere1024$embedding) 
data_cohere1024$embedding = gsub("\\]", "", data_cohere1024$embedding)
data_cohere1024$embedding = gsub(",", " ", data_cohere1024$embedding)

data_cohere1024 = data_cohere1024 |> 
  mutate(embedding = str_trim(embedding),  # Remove leading and trailing spaces
         embedding = str_replace_all(embedding, "\\s+", " ")) |> # Replace multiple spaces with a single space
  separate(embedding, into = paste0("var", 1:1024), sep = "\\s+", convert = TRUE) |>
  mutate(across(starts_with("var"), as.numeric))
  

# Cohere with 50 embedding dimension
data_cohere50 = read.csv("./data2/data_wide_embedding_cohere50.csv") |>
  janitor::clean_names() |>
  dplyr::select(-x,-combined,-n_tokens)
```


### Get entropy - eval=FALSE

```{r entropy, eval=FALSE}
# Entropy
data_entropy = read.csv("./data/data_wide_entropy.csv") |>
  janitor::clean_names() |>
  dplyr::select(-x) 

# GPT1536 + entropy
data_gpt1536_entropy = data_entropy |>
  dplyr::select(seqn, entropy_day1:entropy_day7) |> 
  inner_join(data_gpt1536, by = "seqn")

# GPT50 + entropy
data_gpt50_entropy = data_entropy |>
  dplyr::select(seqn, entropy_day1:entropy_day7) |> 
  inner_join(data_gpt50, by = "seqn")

# BERT768 + entropy
data_bert768_entropy = data_entropy |>
  dplyr::select(seqn, entropy_day1:entropy_day7) |> 
  inner_join(data_bert768, by = "seqn")

# BERT50 + entropy
data_bert50_entropy = data_entropy |>
  dplyr::select(seqn, entropy_day1:entropy_day7) |> 
  inner_join(data_bert50, by = "seqn")

# Cohere1024 + entropy
data_cohere1024_entropy = data_entropy |>
  dplyr::select(seqn, entropy_day1:entropy_day7) |> 
  inner_join(data_cohere1024, by = "seqn")

# Cohere50 + entropy
data_cohere50_entropy = data_entropy |>
  dplyr::select(seqn, entropy_day1:entropy_day7) |> 
  inner_join(data_cohere50, by = "seqn")
```


### Get MOMENT embeddings and combine the subsets 

```{r import moment embeddings}
# MOMENT with 1024 embedding dimension  
data_moment1_1024 = read.csv("./data/embeddings_moment_subset1_1024.csv") |>
  janitor::clean_names() |>
  select(-x) |>
  arrange(seqn) 

data_moment2_1024 = read.csv("./data/embeddings_moment_subset2_1024.csv") |>
  janitor::clean_names() |>
  select(-x) |>
  arrange(seqn) 

# Fix the renaming function to handle all column names at once
data_moment1024 <- rbind(data_moment1_1024, data_moment2_1024) |>
  rename_with(
    .fn = function(x) {
      new_names <- x
      x_cols <- grepl("^x\\d+$", x)
      
      if(any(x_cols)) {
        nums <- as.integer(gsub("x", "", x[x_cols])) + 1
        new_names[x_cols] <- paste0("var", nums)
      }
      return(new_names)
    }
  )


# MOMENT with 50 embedding dimension 
data_moment1_50 = read_csv("./data/embeddings_moment_subset1_50.csv") |>
  janitor::clean_names() |>
  select(-x1) |>
  rename(x1 = x1_2) |>
  arrange(seqn) 

data_moment2_50 = read_csv("./data/embeddings_moment_subset2_50.csv") |>
  janitor::clean_names() |>
  select(-x1) |>
  rename(x1 = x1_2) |>
  arrange(seqn) 

data_moment50 = rbind(data_moment1_50, data_moment2_50) 
```



## Comparing the embeddings through visualization 

```{r visualization with reduced dimension of 50}
# combine the 50 dimension data without entropy 
data_combined50 = bind_rows(data_moment50, 
                            data_gpt50, 
                            data_bert50, 
                            data_cohere50)
data_scaled50 = scale(data_combined50)
labels = c(
  rep("MOMENT", nrow(data_moment50)),
  rep("EntroGPT", nrow(data_gpt50)),
  rep("EntroBERT", nrow(data_bert50)), 
  rep("EntroCohere", nrow(data_cohere50)))


# PCA 
pca_all50 = prcomp(data_scaled50, scale. = TRUE)
pca_df = as.data.frame(pca_all50$x[, 1:2])
pca_df$Model = labels

ggplot(pca_df, aes(x = PC1, y = PC2, color = Model)) +
  geom_point(alpha = 0.6) +
  labs(title = "PCA of Embeddings from MOMENT, GPT, BERT, and Cohere with 50 dimensions",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()


# t-SNE with 50 dimension 
set.seed(1)
data_scaled50_noise = data_scaled50 + matrix(
  rnorm(nrow(data_scaled50) * ncol(data_scaled50), mean = 0, sd = 1e-6),
  nrow = nrow(data_scaled50), ncol = ncol(data_scaled50)
)

tsne_result50 = Rtsne(as.matrix(data_scaled50_noise), dims = 2, perplexity = 30)

tsne_df = as.data.frame(tsne_result50$Y)
tsne_df$model = labels

ggplot(tsne_df, aes(x = V1, y = V2, color = model)) +
  geom_point(alpha = 1) +
  labs(title = "t-SNE of Truncated Embeddings",
       x = "t-SNE 1", y = "t-SNE 2") +
  theme_minimal()
```



```{r PCA with reduced dimension of 768}
# reducing the full embedding data to the lowest dimension (768 of EntroBert) 
data_moment1024_red = data_moment1024[, 1:776]
data_gpt1536_red = data_gpt1536[, 1:776]
data_cohere1024_red = data_cohere1024[, 1:776]

# combine the data 
data_combined768 = bind_rows(data_moment1024_red, 
                            data_gpt1536_red, 
                            data_bert768, 
                            data_cohere1024_red)
data_scaled768 = scale(data_combined768)

pca_all768 = prcomp(data_scaled768, scale. = TRUE)
pca_df_2 = as.data.frame(pca_all768$x[, 1:2])
pca_df_2$Model = labels

ggplot(pca_df_2, aes(x = PC1, y = PC2, color = Model)) +
  geom_point(alpha = 0.6) +
  labs(title = "PCA of Embeddings from MOMENT, GPT, BERT, and Cohere with 768 dimensions",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()


# t-SNE with 768 dimension 
set.seed(1)
data_scaled768_noise = data_scaled768 + matrix(
  rnorm(nrow(data_scaled768) * ncol(data_scaled768), mean = 0, sd = 1e-6),
  nrow = nrow(data_scaled768), ncol = ncol(data_scaled768)
)

tsne_result768 = Rtsne(as.matrix(data_scaled768_noise), dims = 2, perplexity = 30)

tsne_df2 = as.data.frame(tsne_result768$Y)
tsne_df2$model = labels

ggplot(tsne_df2, aes(x = V1, y = V2, color = model)) +
  geom_point(alpha = 1) +
  labs(title = "t-SNE of Truncated Embeddings",
       x = "t-SNE 1", y = "t-SNE 2") +
  theme_minimal()
```


For both 50 dimension and 768 dimensions, 
1) Each model cluster is distinct and separated. 
2) MOMENT embeddings show more compact and curved cluster than the Entro models - this implies that MOMENT captures highly structured and temporally consistent patterns.  
3) GPT and Cohere are more spread out and BERT is more dense and compact amongst the three. 


## Try several simulations for t-SNE
```{r}
tsne_runs50 = lapply(1:10, function(seed) {
  set.seed(seed)
  Rtsne(scale(data_combined50), dims = 2, perplexity = 30)$Y
})

ggplot(tsne_runs50, aes(x = V1, y = V2, color = model)) +
  geom_point(alpha = 1) +
  labs(title = "t-SNE of Truncated Embeddings",
       x = "t-SNE 1", y = "t-SNE 2") +
  theme_minimal()
```

