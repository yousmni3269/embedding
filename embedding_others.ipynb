{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22aefb9-6c8a-4a7a-8b75-130b86302ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m519.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "Collecting openai\n",
      "  Downloading openai-1.71.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Downloading openai-1.71.0-py3-none-any.whl (598 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.0/599.0 kB\u001b[0m \u001b[31m896.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (319 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.9.0 openai-1.71.0\n"
     ]
    }
   ],
   "source": [
    "# installation first\n",
    "!pip install tiktoken\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21eb02fc-3112-4734-81cc-90b596b2496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0c8dcd-a2db-4d13-ba43-18ef25659bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>pir</th>\n",
       "      <th>bmi</th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>...</th>\n",
       "      <th>time2007</th>\n",
       "      <th>time2008</th>\n",
       "      <th>time2009</th>\n",
       "      <th>time2010</th>\n",
       "      <th>time2011</th>\n",
       "      <th>time2012</th>\n",
       "      <th>time2013</th>\n",
       "      <th>time2014</th>\n",
       "      <th>time2015</th>\n",
       "      <th>time2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21009</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21010</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21012</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqn  gender  age  race  education  marital_status   pir  bmi  time1  \\\n",
       "1  21009       1   55     3          3               1  3.79    1      0   \n",
       "2  21010       2   52     3          4               6  1.24    1      0   \n",
       "3  21012       1   63     4          3               6  0.89    0      1   \n",
       "\n",
       "   time2  ...  time2007  time2008  time2009  time2010  time2011  time2012  \\\n",
       "1      0  ...         0         0         0         0         0         0   \n",
       "2      0  ...         0         0         0         0         0         0   \n",
       "3      0  ...         1         1         0         0         0         1   \n",
       "\n",
       "   time2013  time2014  time2015  time2016  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         1         0         0         0  \n",
       "\n",
       "[3 rows x 2024 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import NHANES data \n",
    "df = pd.read_csv(\"data/data_wide.csv\", index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2323f89c-9e69-462c-8bcc-5f8fcf4852cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = [col for col in df.columns if col.startswith('Time')]\n",
    "\n",
    "df[time_columns] = df[time_columns].astype(str)\n",
    "\n",
    "def combine(row):\n",
    "    combined_values = ' '.join(row[col] for col in time_columns)\n",
    "    return combined_values\n",
    "\n",
    "df['combined'] = df.apply(combine, axis=1)\n",
    "\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75bab566-a3c3-4bf3-9e00-2eb0a6061d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[col for col in df.columns if col.startswith('Time')])\n",
    "df.to_csv(\"data/data_wide_embedding_initial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f14e16f6-1ea7-4fd8-b372-59e56e1e4286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>pir</th>\n",
       "      <th>bmi</th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>...</th>\n",
       "      <th>time2009</th>\n",
       "      <th>time2010</th>\n",
       "      <th>time2011</th>\n",
       "      <th>time2012</th>\n",
       "      <th>time2013</th>\n",
       "      <th>time2014</th>\n",
       "      <th>time2015</th>\n",
       "      <th>time2016</th>\n",
       "      <th>combined</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21009</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21010</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 2026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqn  gender  age  race  education  marital_status   pir  bmi  time1  \\\n",
       "1  21009       1   55     3          3               1  3.79    1      0   \n",
       "2  21010       2   52     3          4               6  1.24    1      0   \n",
       "\n",
       "   time2  ...  time2009  time2010  time2011  time2012  time2013  time2014  \\\n",
       "1      0  ...         0         0         0         0         0         0   \n",
       "2      0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   time2015  time2016  combined  n_tokens  \n",
       "1         0         0       NaN         0  \n",
       "2         0         0       NaN         0  \n",
       "\n",
       "[2 rows x 2026 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data_wide_embedding_initial.csv\", index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f16d08-a517-4b2c-a47d-764c3f386afa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ECG5000_TRAIN.ts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m os.makedirs(\u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Move or copy your downloaded files to the expected location\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Assuming your downloaded files are in the current directory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mECG5000_TRAIN.ts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/ECG5000_TRAIN.ts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m shutil.copy(\u001b[33m'\u001b[39m\u001b[33mECG5000_TEST.ts\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m../data/ECG5000_TEST.ts\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/shutil.py:431\u001b[39m, in \u001b[36mcopy\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(dst):\n\u001b[32m    430\u001b[39m     dst = os.path.join(dst, os.path.basename(src))\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m copymode(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/shutil.py:256\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    254\u001b[39m     os.symlink(os.readlink(src), dst)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[32m    259\u001b[39m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'ECG5000_TRAIN.ts'"
     ]
    }
   ],
   "source": [
    "################### GPT ##############################\n",
    "\n",
    "# model: text-embedding-3-small\n",
    "# default embedding dimension: 1536\n",
    "\n",
    "from openai import OpenAI\n",
    "api_key = ''\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[ 0].embedding\n",
    "\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8191  # the maximum for text-embedding-3-small is 8191\n",
    "\n",
    "# default dimension\n",
    "\n",
    "df_gpt=df.copy()\n",
    "df_gpt[\"embedding\"] = df_gpt.combined.apply(lambda x: get_embedding(x, model=embedding_model))\n",
    "df_gpt = df_gpt[[col for col in df.columns if not col.startswith('n_tokens') and not col.startswith('Time')]]\n",
    "df_gpt.to_csv(\"E:/EntroLLM/data_wide_embedding_gpt1536.csv\")\n",
    "df_gpt.head(2)\n",
    "\n",
    "# reduced dimension\n",
    "\n",
    "# Normalize embeddings using L2 normalization\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "def get_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = client.embeddings.create(\n",
    "            model=model, input=text, encoding_format=\"float\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding[:50])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def batch_process(texts, batch_size=10):\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield texts[i:i+batch_size]\n",
    "\n",
    "SEQN = df['SEQN'].tolist()\n",
    "batched_texts = list(batch_process(df['combined']))\n",
    "all_embeddings = []\n",
    "all_SEQN = []\n",
    "\n",
    "for batch in batched_texts:\n",
    "    embeddings = get_embeddings(batch)\n",
    "    all_embeddings.extend(embeddings)\n",
    "    start_index = len(all_SEQN)\n",
    "    all_SEQN.extend(SEQN[start_index:start_index + len(batch)])\n",
    "    \n",
    "# Convert all embeddings to numpy array and normalize\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "norm_dim = normalize_l2(all_embeddings)\n",
    "\n",
    "# Create a dataframe from the normalized embeddings\n",
    "df_embeddings = pd.DataFrame(norm_dim)\n",
    "df_embeddings['SEQN'] = all_SEQN\n",
    "\n",
    "# Reorder columns to have SEQN as the first column\n",
    "cols = df_embeddings.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_embeddings = df_embeddings[cols]\n",
    "\n",
    "df_filtered = df.loc[:, ~df.columns.str.startswith('Time')]\n",
    "\n",
    "# merge two dataset\n",
    "df_new = pd.merge(df_filtered, df_embeddings, on='SEQN', how='left')\n",
    "\n",
    "df_new.to_csv('E:/EntroLLM/data_wide_embedding_gpt50.csv', index=False)\n",
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e7d409-8ea2-4c75-bde3-b24fc67aabf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ECG5000_TRAIN.ts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmomentfm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ClassificationDataset\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_dataset = \u001b[43mClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_split\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m test_dataset = ClassificationDataset(data_split=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/momentfm/data/classification_dataset.py:22\u001b[39m, in \u001b[36mClassificationDataset.__init__\u001b[39m\u001b[34m(self, data_split)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.data_split = data_split  \u001b[38;5;66;03m# 'train' or 'test'\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Read data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/momentfm/data/classification_dataset.py:41\u001b[39m, in \u001b[36mClassificationDataset._read_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_data, \u001b[38;5;28mself\u001b[39m.train_labels = \u001b[43mload_from_tsfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_file_path_and_name\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m.test_data, \u001b[38;5;28mself\u001b[39m.test_labels = load_from_tsfile(\n\u001b[32m     45\u001b[39m         \u001b[38;5;28mself\u001b[39m.test_file_path_and_name\n\u001b[32m     46\u001b[39m     )\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_labels, \u001b[38;5;28mself\u001b[39m.test_labels = \u001b[38;5;28mself\u001b[39m._transform_labels(\n\u001b[32m     49\u001b[39m         \u001b[38;5;28mself\u001b[39m.train_labels, \u001b[38;5;28mself\u001b[39m.test_labels\n\u001b[32m     50\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/momentfm/utils/data.py:46\u001b[39m, in \u001b[36mload_from_tsfile\u001b[39m\u001b[34m(full_file_path_and_name, replace_missing_vals_with, return_meta_data, return_type)\u001b[39m\n\u001b[32m     44\u001b[39m     full_file_path_and_name = full_file_path_and_name + \u001b[33m\"\u001b[39m\u001b[33m.ts\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Open file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfull_file_path_and_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Read in headers\u001b[39;00m\n\u001b[32m     48\u001b[39m     meta_data = _load_header_info(file)\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# load into list of numpy\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/ECG5000_TRAIN.ts'"
     ]
    }
   ],
   "source": [
    "# GPT embedding modified \n",
    "\n",
    "from openai import OpenAI\n",
    "api_key = ''\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    if not isinstance(text, str):  # Handle non-string inputs\n",
    "        return None\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    \n",
    "#def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   #text = text.replace(\"\\n\", \" \")\n",
    "   #return client.embeddings.create(input = [text], model=model).data[ 0].embedding\n",
    "\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8191  # the maximum for text-embedding-3-small is 8191\n",
    "\n",
    "# default dimension\n",
    "\n",
    "df_gpt=df.copy()\n",
    "df_gpt[\"embedding\"] = df_gpt.combined.apply(lambda x: get_embedding(x, model=embedding_model))\n",
    "df_gpt = df_gpt[[col for col in df.columns if not col.startswith('n_tokens') and not col.startswith('Time')]]\n",
    "df_gpt.to_csv(\"data/data_wide_embedding_gpt1536.csv\")\n",
    "df_gpt.head(2)\n",
    "\n",
    "# reduced dimension\n",
    "\n",
    "# Normalize embeddings using L2 normalization\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "def get_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = client.embeddings.create(\n",
    "            model=model, input=text, encoding_format=\"float\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding[:50])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def batch_process(texts, batch_size=10):\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield texts[i:i+batch_size]\n",
    "\n",
    "SEQN = df['seqn'].tolist()\n",
    "batched_texts = list(batch_process(df['combined']))\n",
    "all_embeddings = []\n",
    "all_SEQN = []\n",
    "\n",
    "for batch in batched_texts:\n",
    "    embeddings = get_embeddings(batch)\n",
    "    all_embeddings.extend(embeddings)\n",
    "    start_index = len(all_seqn)\n",
    "    all_SEQN.extend(seqn[start_index:start_index + len(batch)])\n",
    "    \n",
    "# Convert all embeddings to numpy array and normalize\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "norm_dim = normalize_l2(all_embeddings)\n",
    "\n",
    "# Create a dataframe from the normalized embeddings\n",
    "df_embeddings = pd.DataFrame(norm_dim)\n",
    "df_embeddings['seqn'] = all_SEQN\n",
    "\n",
    "# Reorder columns to have SEQN as the first column\n",
    "cols = df_embeddings.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_embeddings = df_embeddings[cols]\n",
    "\n",
    "df_filtered = df.loc[:, ~df.columns.str.startswith('time')]\n",
    "\n",
    "# merge two dataset\n",
    "df_new = pd.merge(df_filtered, df_embeddings, on='seqn', how='left')\n",
    "\n",
    "df_new.to_csv('data/data_wide_embedding_gpt50.csv', index=False)\n",
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7feada4b-0ee1-46a7-8d5d-4bf3e1b8c166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py311/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd3031eea5c4509883eed2d183588d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5115e52d29b43c8bcd9c4400b3369c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70387db2e794457fad8c38ec7044fb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474693f87e664fb68b197c1c002534ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_embedding.squeeze().detach().numpy()  \u001b[38;5;66;03m# Shape: (768,)\u001b[39;00m\n\u001b[32m     32\u001b[39m df_bert=df.copy()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m df_bert[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf_bert\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcombined\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_bert_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m columns_to_drop = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_bert.columns \u001b[38;5;28;01mif\u001b[39;00m col.startswith(\u001b[33m'\u001b[39m\u001b[33mn_tokens\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col.startswith(\u001b[33m'\u001b[39m\u001b[33mTime\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m     36\u001b[39m df_bert = df_bert.drop(columns=columns_to_drop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mget_bert_embeddings\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_bert_embeddings\u001b[39m(text):\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Convert the text to the model input format\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     inputs = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Disable gradient calculation\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     25\u001b[39m         \u001b[38;5;66;03m# Get the model outputs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2602\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2600\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2601\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2602\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2604\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2660\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2657\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[32m-> \u001b[39m\u001b[32m2660\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2662\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2663\u001b[39m     )\n\u001b[32m   2665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2667\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2668\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2669\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "################### BERT ##############################\n",
    "\n",
    "# model: bert-base-uncased\n",
    "# default embedding dimension: 768\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# default dimension\n",
    "\n",
    "# Define a function to get BERT embeddings\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    # Convert the text to the model input format\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Get the model outputs\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the CLS token embeddings\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, 768)\n",
    "    return cls_embedding.squeeze().detach().numpy()  # Shape: (768,)\n",
    "\n",
    "df_bert=df.copy()\n",
    "df_bert[\"embedding\"] = df_bert['combined'].apply(get_bert_embeddings)\n",
    "\n",
    "columns_to_drop = [col for col in df_bert.columns if col.startswith('n_tokens') or col.startswith('Time')]\n",
    "df_bert = df_bert.drop(columns=columns_to_drop)\n",
    "\n",
    "df_bert.to_csv(\"E:/EntroLLM/data_wide_embedding_bert768.csv\")\n",
    "df_bert.head(2)\n",
    "\n",
    "# reduce dimension\n",
    "\n",
    "\n",
    "def reduce_dimension(embedding, dim=50):\n",
    "    return embedding[:, :dim]\n",
    "\n",
    "# Normalize embeddings using L2 normalization\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "\n",
    "# Convert texts to embedding and reduce the dimension\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (1, 768)\n",
    "    reduced_embedding = reduce_dimension(cls_embedding)  # Shape: (1, 50)\n",
    "    return reduced_embedding.squeeze().detach().numpy()  # Shape: (50,)\n",
    "\n",
    "\n",
    "# Process texts in batches\n",
    "def batch_process(texts, batch_size=10):\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield texts[i:i+batch_size]\n",
    "\n",
    "SEQN = df['SEQN'].tolist()\n",
    "batched_texts = list(batch_process(df['combined']))\n",
    "all_embeddings = []\n",
    "all_SEQN = []\n",
    "\n",
    "\n",
    "for batch in batched_texts:\n",
    "    batch_embeddings = []\n",
    "    for text in batch:\n",
    "        embedding = get_bert_embeddings(text)\n",
    "        batch_embeddings.append(embedding)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    start_index = len(all_SEQN)\n",
    "    all_SEQN.extend(SEQN[start_index:start_index + len(batch)])\n",
    "\n",
    "\n",
    "# Convert all embeddings to numpy array and normalize\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "norm_dim = normalize_l2(all_embeddings)\n",
    "\n",
    "# Create a dataframe from the normalized embeddings\n",
    "df_embeddings = pd.DataFrame(norm_dim)\n",
    "df_embeddings['SEQN'] = all_SEQN\n",
    "\n",
    "# Reorder columns to have SEQN as the first column\n",
    "cols = df_embeddings.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_embeddings = df_embeddings[cols]\n",
    "\n",
    "df_filtered = df.loc[:, ~df.columns.str.startswith('Time')]\n",
    "\n",
    "# merge two dataset\n",
    "df_bert = pd.merge(df_filtered, df_embeddings, on='SEQN', how='left')\n",
    "\n",
    "df_bert.to_csv(\"E:/EntroLLM/data_wide_embedding_bert50.csv\")\n",
    "df_bert.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "098ed66e-401e-4aab-859b-b9296dd4dc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>pir</th>\n",
       "      <th>bmi</th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>...</th>\n",
       "      <th>time2007</th>\n",
       "      <th>time2008</th>\n",
       "      <th>time2009</th>\n",
       "      <th>time2010</th>\n",
       "      <th>time2011</th>\n",
       "      <th>time2012</th>\n",
       "      <th>time2013</th>\n",
       "      <th>time2014</th>\n",
       "      <th>time2015</th>\n",
       "      <th>time2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21009</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21010</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21012</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqn  gender  age  race  education  marital_status   pir  bmi  time1  \\\n",
       "1  21009       1   55     3          3               1  3.79    1      0   \n",
       "2  21010       2   52     3          4               6  1.24    1      0   \n",
       "3  21012       1   63     4          3               6  0.89    0      1   \n",
       "\n",
       "   time2  ...  time2007  time2008  time2009  time2010  time2011  time2012  \\\n",
       "1      0  ...         0         0         0         0         0         0   \n",
       "2      0  ...         0         0         0         0         0         0   \n",
       "3      0  ...         1         1         0         0         0         1   \n",
       "\n",
       "   time2013  time2014  time2015  time2016  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         1         0         0         0  \n",
       "\n",
       "[3 rows x 2024 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### Cohere ##############################\n",
    "\n",
    "import time\n",
    "import cohere\n",
    "co = cohere.Client(\"\") \n",
    "\n",
    "\n",
    "#####################\n",
    "# split into two dataframe and run them separetly if needed\n",
    "\n",
    "# num_rows = len(df)\n",
    "# print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "# split_point = num_rows // 2\n",
    "\n",
    "# df1 = df.iloc[:split_point]\n",
    "# df2 = df.iloc[split_point:]\n",
    "#######################\n",
    "\n",
    "\n",
    "# model: embed-english-v3.0\n",
    "# default embedding dimension: 1024\n",
    "\n",
    "# default dimension\n",
    "\n",
    "def get_embeddings(texts, model='embed-english-v3.0', input_type=\"search_document\"):\n",
    "    output = co.embed(\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        texts=texts)\n",
    "    return output.embeddings\n",
    "\n",
    "# Create a new DataFrame to store embeddings\n",
    "df_cohere = df.drop(columns=['combined']).copy()\n",
    "df_cohere['embedding'] = get_embeddings(df['combined'].tolist())\n",
    "\n",
    "df_cohere.to_csv(\"E:/EntroLLM/data_wide_embedding_cohere1024.csv\")\n",
    "\n",
    "\n",
    "# reduce dimension\n",
    "\n",
    "def reduce_dimension(embedding, dim=50):\n",
    "    return embedding[:, :dim]\n",
    "\n",
    "\n",
    "# Normalize embeddings using L2 normalization\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "# Get embeddings for given texts\n",
    "def get_embeddings(texts, model='embed-english-v3.0', input_type=\"search_document\",delay=0):\n",
    "    output = co.embed(\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        texts=texts)\n",
    "    time.sleep(delay)\n",
    "    reduced_embedding = reduce_dimension(np.array(output.embeddings))  # Shape: (batch_size, dim)\n",
    "    return reduced_embedding\n",
    "\n",
    "# Process texts in batches\n",
    "def batch_process(texts, batch_size=10):\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield texts[i:i+batch_size]\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "SEQN = df['SEQN'].tolist()\n",
    "batched_texts = list(batch_process(df['combined'].tolist()))\n",
    "all_embeddings = []\n",
    "all_SEQN = []\n",
    "\n",
    "for batch in batched_texts:\n",
    "    embeddings = get_embeddings(batch)\n",
    "    all_embeddings.append(embeddings)\n",
    "    start_index = len(all_SEQN)\n",
    "    all_SEQN.extend(SEQN[start_index:start_index + len(batch)])\n",
    "\n",
    "# Convert all embeddings to numpy array and normalize\n",
    "all_embeddings = np.vstack(all_embeddings)  # Stack embeddings vertically\n",
    "norm_dim = normalize_l2(all_embeddings)  \n",
    "\n",
    "# Create a dataframe from the normalized embeddings\n",
    "df_embeddings = pd.DataFrame(norm_dim)\n",
    "df_embeddings['SEQN'] = all_SEQN\n",
    "\n",
    "# Reorder columns to have SEQN as the first column\n",
    "cols = df_embeddings.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_embeddings = df_embeddings[cols]\n",
    "\n",
    "df_filtered = df.loc[:, ~df.columns.str.startswith('Time')]\n",
    "\n",
    "# Merge two datasets\n",
    "df_cohere = pd.merge(df_filtered, df_embeddings, on='SEQN', how='left')\n",
    "\n",
    "df_cohere.to_csv(\"E:/EntroLLM/data_wide_embedding_cohere50.csv\")\n",
    "df_cohere.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
